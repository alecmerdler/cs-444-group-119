From 033a618ebbe0ca197cad0255b6a7f1b7c871c09a Mon Sep 17 00:00:00 2001
From: Leon Leighton <leightle@oregonstate.edu>
Date: Fri, 9 Jun 2017 12:36:29 -0700
Subject: [PATCH 1/2] hw4

---
 arch/x86/syscalls/syscall_32.tbl |  2 ++
 include/linux/syscalls.h         |  2 ++
 mm/slob.c                        | 59 +++++++++++++++++++++++++++++++---------
 3 files changed, 50 insertions(+), 13 deletions(-)

diff --git a/arch/x86/syscalls/syscall_32.tbl b/arch/x86/syscalls/syscall_32.tbl
index 96bc506..ce28be3 100644
--- a/arch/x86/syscalls/syscall_32.tbl
+++ b/arch/x86/syscalls/syscall_32.tbl
@@ -359,3 +359,5 @@
 350	i386	finit_module		sys_finit_module
 351	i386	sched_setattr		sys_sched_setattr
 352	i386	sched_getattr		sys_sched_getattr
+353	i386	mem_free		sys_mem_free
+354	i386	mem_used		sys_mem_used
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index a747a77..a12c097 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -855,4 +855,6 @@ asmlinkage long sys_process_vm_writev(pid_t pid,
 asmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,
 			 unsigned long idx1, unsigned long idx2);
 asmlinkage long sys_finit_module(int fd, const char __user *uargs, int flags);
+asmlinkage long sys_mem_free(void);
+asmlinkage long sys_mem_used(void);
 #endif
diff --git a/mm/slob.c b/mm/slob.c
index 4bf8809..ed7c2cc 100644
--- a/mm/slob.c
+++ b/mm/slob.c
@@ -71,7 +71,7 @@
 #include <trace/events/kmem.h>
 
 #include <linux/atomic.h>
-
+#include <linux/syscalls.h>
 #include "slab.h"
 /*
  * slob_block has a field 'units', which indicates size of block if +ve,
@@ -92,6 +92,10 @@ struct slob_block {
 };
 typedef struct slob_block slob_t;
 
+/* Used for sys calls */
+unsigned long mem_slob_free = 0;
+unsigned long mem_slob_count = 0;
+
 /*
  * All partially free slob pages go on these lists.
  */
@@ -268,6 +272,7 @@ static void *slob_page_alloc(struct page *sp, size_t size, int align)
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
 	struct page *sp;
+	struct page *sp_best = NULL;
 	struct list_head *prev;
 	struct list_head *slob_list;
 	slob_t *b = NULL;
@@ -294,21 +299,32 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 		/* Enough room on this page? */
 		if (sp->units < SLOB_UNITS(size))
 			continue;
+		
+		/* If we have page with room, check if it's the best so far 
+ 		 * If the new page (sp->units) is smaller than what we have
+ 		 * (sp_best->units) then it is a better fit. 
+		 */
+		if ((sp_best == NULL) || (sp_best->units > sp->units))
+			sp_best = sp;
 
-		/* Attempt to alloc */
-		prev = sp->list.prev;
-		b = slob_page_alloc(sp, size, align);
-		if (!b)
-			continue;
+	}
+	/* Attempt to alloc */
+	if (sp_best != NULL) {
+		b = slob_page_alloc(sp_best, size, align);
+	}
 
-		/* Improve fragment distribution and reduce our average
-		 * search time by starting our next search here. (see
-		 * Knuth vol 1, sec 2.5, pg 449) */
-		if (prev != slob_list->prev &&
-				slob_list->next != prev->next)
-			list_move_tail(slob_list, prev->next);
-		break;
+	/* Recalculate free space */
+	mem_slob_free = 0;
+	list_for_each_entry(sp, &free_slob_small, list) {
+		mem_slob_free += sp->units;
+	}
+	list_for_each_entry(sp, &free_slob_medium, list) {
+		mem_slob_free += sp->units;
 	}
+	list_for_each_entry(sp, &free_slob_large, list) {
+		mem_slob_free += sp->units;
+	}
+
 	spin_unlock_irqrestore(&slob_lock, flags);
 
 	/* Not enough space: must allocate a new page */
@@ -326,6 +342,7 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 		set_slob(b, SLOB_UNITS(PAGE_SIZE), b + SLOB_UNITS(PAGE_SIZE));
 		set_slob_page_free(sp, slob_list);
 		b = slob_page_alloc(sp, size, align);
+		mem_slob_count++; // new page
 		BUG_ON(!b);
 		spin_unlock_irqrestore(&slob_lock, flags);
 	}
@@ -362,6 +379,7 @@ static void slob_free(void *block, int size)
 		__ClearPageSlab(sp);
 		page_mapcount_reset(sp);
 		slob_free_pages(b, 0);
+		mem_slob_count--;
 		return;
 	}
 
@@ -420,6 +438,21 @@ out:
 }
 
 /*
+ * system calls
+ */
+
+SYSCALL_DEFINE0(mem_free)
+{
+	return mem_slob_free;
+}
+
+SYSCALL_DEFINE0(mem_used)
+{
+
+	return (SLOB_UNITS(PAGE_SIZE) * mem_slob_count);
+}
+
+/*
  * End of slob allocator proper. Begin kmem_cache_alloc and kmalloc frontend.
  */
 
-- 
1.7.12.4


From e74ad98a92d77716201926720bb90d947f13ef30 Mon Sep 17 00:00:00 2001
From: Leon Leighton <leightle@oregonstate.edu>
Date: Fri, 9 Jun 2017 13:56:12 -0700
Subject: [PATCH 2/2] change syscalls

---
 arch/x86/syscalls/syscall_32.tbl |  2 +-
 include/linux/syscalls.h         |  2 +-
 mm/slob.c                        | 20 ++++++++++++--------
 3 files changed, 14 insertions(+), 10 deletions(-)

diff --git a/arch/x86/syscalls/syscall_32.tbl b/arch/x86/syscalls/syscall_32.tbl
index ce28be3..43fd3eb 100644
--- a/arch/x86/syscalls/syscall_32.tbl
+++ b/arch/x86/syscalls/syscall_32.tbl
@@ -359,5 +359,5 @@
 350	i386	finit_module		sys_finit_module
 351	i386	sched_setattr		sys_sched_setattr
 352	i386	sched_getattr		sys_sched_getattr
-353	i386	mem_free		sys_mem_free
+353	i386	mem_size		sys_mem_size
 354	i386	mem_used		sys_mem_used
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index a12c097..be02f21 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -855,6 +855,6 @@ asmlinkage long sys_process_vm_writev(pid_t pid,
 asmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,
 			 unsigned long idx1, unsigned long idx2);
 asmlinkage long sys_finit_module(int fd, const char __user *uargs, int flags);
-asmlinkage long sys_mem_free(void);
+asmlinkage long sys_mem_size(void);
 asmlinkage long sys_mem_used(void);
 #endif
diff --git a/mm/slob.c b/mm/slob.c
index ed7c2cc..09552c5 100644
--- a/mm/slob.c
+++ b/mm/slob.c
@@ -93,8 +93,8 @@ struct slob_block {
 typedef struct slob_block slob_t;
 
 /* Used for sys calls */
-unsigned long mem_slob_free = 0;
-unsigned long mem_slob_count = 0;
+unsigned long mem_slob_size = 0;
+unsigned long mem_slob_used = 0;
 
 /*
  * All partially free slob pages go on these lists.
@@ -205,6 +205,7 @@ static void *slob_new_pages(gfp_t gfp, int order, int node)
 	if (!page)
 		return NULL;
 
+	mem_slob_size += PAGE_SIZE;
 	return page_address(page);
 }
 
@@ -212,6 +213,8 @@ static void slob_free_pages(void *b, int order)
 {
 	if (current->reclaim_state)
 		current->reclaim_state->reclaimed_slab += 1 << order;
+
+	mem_slob_size -= PAGE_SIZE;
 	free_pages((unsigned long)b, order);
 }
 
@@ -259,6 +262,7 @@ static void *slob_page_alloc(struct page *sp, size_t size, int align)
 			sp->units -= units;
 			if (!sp->units)
 				clear_slob_page_free(sp);
+				mem_slob_used += units;
 			return cur;
 		}
 		if (slob_last(cur))
@@ -313,7 +317,7 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 		b = slob_page_alloc(sp_best, size, align);
 	}
 
-	/* Recalculate free space */
+	/* Recalculate free space 
 	mem_slob_free = 0;
 	list_for_each_entry(sp, &free_slob_small, list) {
 		mem_slob_free += sp->units;
@@ -324,6 +328,7 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 	list_for_each_entry(sp, &free_slob_large, list) {
 		mem_slob_free += sp->units;
 	}
+	*/
 
 	spin_unlock_irqrestore(&slob_lock, flags);
 
@@ -342,7 +347,6 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 		set_slob(b, SLOB_UNITS(PAGE_SIZE), b + SLOB_UNITS(PAGE_SIZE));
 		set_slob_page_free(sp, slob_list);
 		b = slob_page_alloc(sp, size, align);
-		mem_slob_count++; // new page
 		BUG_ON(!b);
 		spin_unlock_irqrestore(&slob_lock, flags);
 	}
@@ -379,7 +383,6 @@ static void slob_free(void *block, int size)
 		__ClearPageSlab(sp);
 		page_mapcount_reset(sp);
 		slob_free_pages(b, 0);
-		mem_slob_count--;
 		return;
 	}
 
@@ -405,6 +408,7 @@ static void slob_free(void *block, int size)
 	 * point.
 	 */
 	sp->units += units;
+	mem_slob_used -= units;
 
 	if (b < (slob_t *)sp->freelist) {
 		if (b + units == sp->freelist) {
@@ -441,15 +445,15 @@ out:
  * system calls
  */
 
-SYSCALL_DEFINE0(mem_free)
+SYSCALL_DEFINE0(mem_size)
 {
-	return mem_slob_free;
+	return mem_slob_size;
 }
 
 SYSCALL_DEFINE0(mem_used)
 {
 
-	return (SLOB_UNITS(PAGE_SIZE) * mem_slob_count);
+	return mem_slob_used;
 }
 
 /*
-- 
1.7.12.4

